{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, AutoencoderKL, CLIPTextModel, CLIPTokenizer\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from safetensors.torch import save_file\n",
    "from huggingface_hub import HfApi, upload_folder\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "sd_pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    sd_model_id,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "# Move components to device\n",
    "sd_pipeline.to(device)\n",
    "\n",
    "# Extract components for fine-tuning\n",
    "vae = sd_pipeline.vae\n",
    "text_encoder = sd_pipeline.text_encoder\n",
    "tokenizer = sd_pipeline.tokenizer\n",
    "unet = sd_pipeline.unet\n",
    "\n",
    "# Set to train mode\n",
    "vae.train()\n",
    "text_encoder.train()\n",
    "unet.train()\n",
    "\n",
    "# Load CLIP model for classifier\n",
    "clip_model_id = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(clip_model_id)\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_model_id)\n",
    "clip_model = clip_model.to(device)\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define Transformation for VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_transform = Compose([\n",
    "    Resize(512, interpolation=Image.BICUBIC),\n",
    "    CenterCrop(512),\n",
    "    ToTensor(),\n",
    "    Normalize([0.5], [0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define Embedding Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_embedding(image, clip_processor, clip_model, device):\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        clip_features = clip_model.get_image_features(**inputs)\n",
    "    clip_features = clip_features / clip_features.norm(p=2, dim=-1, keepdim=True)\n",
    "    return clip_features.cpu()\n",
    "\n",
    "def get_vae_embedding(image, vae, vae_transform, device):\n",
    "    image = vae_transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        latent_dist = vae.encode(image).latent_dist\n",
    "        vae_features = latent_dist.mean\n",
    "    return vae_features.cpu()\n",
    "\n",
    "def get_combined_embedding(image, clip_processor, clip_model, vae, vae_transform, device):\n",
    "    clip_emb = get_clip_embedding(image, clip_processor, clip_model, device)\n",
    "    vae_emb = get_vae_embedding(image, vae, vae_transform, device)\n",
    "    combined_emb = torch.cat([clip_emb, vae_emb], dim=1)\n",
    "    return combined_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define Custom Dataset using Hugging Face datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtistDataset(Dataset):\n",
    "    def __init__(self, dataset_name, split, tokenizer, clip_processor, clip_model, vae, vae_transform, label_encoder, max_length=77):\n",
    "        self.dataset = load_dataset(dataset_name, split=split)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.clip_processor = clip_processor\n",
    "        self.clip_model = clip_model\n",
    "        self.vae = vae\n",
    "        self.vae_transform = vae_transform\n",
    "        self.label_encoder = label_encoder\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['image']\n",
    "        artist = item['artist']\n",
    "        prompt = item.get('text', f\"Artwork by {artist}\")  # Assuming 'text' field exists; else use default.\n",
    "\n",
    "        # Tokenize prompt\n",
    "        inputs = self.tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        input_ids = inputs.input_ids.squeeze(0)  # Shape: (max_length,)\n",
    "        attention_mask = inputs.attention_mask.squeeze(0)\n",
    "\n",
    "        # Get combined embedding for classifier\n",
    "        combined_emb = get_combined_embedding(image, self.clip_processor, self.clip_model, self.vae, self.vae_transform, device)\n",
    "\n",
    "        # Get label\n",
    "        label = self.label_encoder.transform([artist])[0]\n",
    "\n",
    "        # Process image for diffusion model\n",
    "        image = self.vae_transform(image)\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'pixel_values': image,\n",
    "            'labels': image,  # For diffusion models, labels are typically the same as pixel_values\n",
    "            'combined_emb': combined_emb.squeeze(0),\n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Initialize Label Encoder and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"your_username/your_dataset_name\"  # Replace with your Hugging Face dataset name\n",
    "dataset = load_dataset(dataset_name, split='train')\n",
    "\n",
    "# Gather artist names\n",
    "artist_names = dataset.unique('artist')\n",
    "\n",
    "# Initialize and fit LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(artist_names)\n",
    "\n",
    "# Save LabelEncoder\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4  # Reduced batch size due to high memory consumption\n",
    "num_workers = 4\n",
    "\n",
    "train_dataset = ArtistDataset(\n",
    "    dataset_name=dataset_name,\n",
    "    split='train',\n",
    "    tokenizer=tokenizer,\n",
    "    clip_processor=clip_processor,\n",
    "    clip_model=clip_model,\n",
    "    vae=vae,\n",
    "    vae_transform=vae_transform,\n",
    "    label_encoder=label_encoder\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Define the Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtistClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ArtistClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc3(x)\n",
    "        return logits\n",
    "\n",
    "# Determine input dimensions\n",
    "clip_dim = clip_model.config.projection_dim if hasattr(clip_model.config, 'projection_dim') else 512\n",
    "vae_dim = vae.config.latent_channels * vae.config.block_out_channels[-1]  # Adjust based on VAE architecture\n",
    "input_dim = clip_dim + vae_dim\n",
    "\n",
    "num_classes = len(artist_names)\n",
    "\n",
    "model = ArtistClassifier(input_dim=input_dim, num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Define Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_unet = nn.MSELoss()  # Typical for diffusion models\n",
    "\n",
    "# Optimizer for classifier\n",
    "optimizer_cls = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Optimizer for Stable Diffusion components (fine-tuning UNet and Text Encoder)\n",
    "optimizer_sd = optim.Adam(\n",
    "    list(unet.parameters()) + list(text_encoder.parameters()),\n",
    "    lr=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Mixed Precision and Gradient Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3  # Adjust based on your needs and resources\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    unet.train()\n",
    "    text_encoder.train()\n",
    "    total_loss_cls = 0\n",
    "    total_loss_unet = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        combined_emb = batch['combined_emb'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer_cls.zero_grad()\n",
    "        optimizer_sd.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            # === Fine-Tuning Stable Diffusion ===\n",
    "            # Encode the text\n",
    "            encoder_hidden_states = text_encoder(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "            # Sample noise and add to images (for diffusion)\n",
    "            noise = torch.randn_like(pixel_values)\n",
    "            timesteps = torch.randint(0, 1000, (pixel_values.shape[0],), device=device).long()\n",
    "            noisy_images = noise.add_0(pixel_values)  # Placeholder for actual noise addition based on timesteps\n",
    "\n",
    "            # Get model prediction\n",
    "            noise_pred = unet(noisy_images, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "            # Compute diffusion loss\n",
    "            loss_unet = criterion_unet(noise_pred, noise)\n",
    "\n",
    "            # === Fine-Tuning Classifier ===\n",
    "            # Forward pass through classifier\n",
    "            logits = model(combined_emb)\n",
    "            loss_cls = criterion_cls(logits, label)\n",
    "\n",
    "            # === Combine Losses ===\n",
    "            # Weight the losses as needed\n",
    "            total_loss = loss_unet + loss_cls\n",
    "\n",
    "        # Backpropagation with mixed precision\n",
    "        scaler.scale(total_loss).backward()\n",
    "        scaler.step(optimizer_sd)\n",
    "        scaler.step(optimizer_cls)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss_unet += loss_unet.item()\n",
    "        total_loss_cls += loss_cls.item()\n",
    "\n",
    "    avg_loss_unet = total_loss_unet / len(train_dataloader)\n",
    "    avg_loss_cls = total_loss_cls / len(train_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss UNet: {avg_loss_unet:.4f}, Loss Classifier: {avg_loss_cls:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Save the Trained Model using safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the classification head\n",
    "save_file(model.state_dict(), \"artist_classifier.safetensors\")\n",
    "print(\"Classifier saved to artist_classifier.safetensors\")\n",
    "\n",
    "# Save the fine-tuned Stable Diffusion model components\n",
    "sd_output_dir = \"fine_tuned_stable_diffusion\"\n",
    "os.makedirs(sd_output_dir, exist_ok=True)\n",
    "\n",
    "# Save UNet\n",
    "unet.save_pretrained(os.path.join(sd_output_dir, \"unet\"))\n",
    "\n",
    "# Save Text Encoder\n",
    "text_encoder.save_pretrained(os.path.join(sd_output_dir, \"text_encoder\"))\n",
    "\n",
    "# Save VAE (if needed)\n",
    "vae.save_pretrained(os.path.join(sd_output_dir, \"vae\"))\n",
    "\n",
    "print(f\"Stable Diffusion components saved to {sd_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Upload the model to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "repo_id_classifier = \"your-username/artist-classifier\"  # Replace with your repository name for classifier\n",
    "repo_id_sd = \"your-username/fine-tuned-stable-diffusion\"  # Replace with your repository name for SD\n",
    "\n",
    "# Create repositories if they don't exist\n",
    "api.create_repo(repo_id_classifier, exist_ok=True)\n",
    "api.create_repo(repo_id_sd, exist_ok=True)\n",
    "\n",
    "# Upload the classifier\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"artist_classifier.safetensors\",\n",
    "    path_in_repo=\"artist_classifier.safetensors\",\n",
    "    repo_id=repo_id_classifier,\n",
    ")\n",
    "\n",
    "# Upload the label encoder\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"label_encoder.pkl\",\n",
    "    path_in_repo=\"label_encoder.pkl\",\n",
    "    repo_id=repo_id_classifier,\n",
    ")\n",
    "\n",
    "# Upload the fine-tuned Stable Diffusion components\n",
    "api.upload_folder(\n",
    "    folder_path=sd_output_dir,\n",
    "    repo_id=repo_id_sd,\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "print(f\"Models uploaded to {repo_id_classifier} and {repo_id_sd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Define Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_with_artist_reference(prompt, sd_pipeline, model, clip_processor, clip_model, vae, vae_transform, label_encoder, device, tokenizer, top_k=3):\n",
    "    # Tokenize prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate Image\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device.type):\n",
    "            generated_image = sd_pipeline(prompt).images[0]\n",
    "\n",
    "    # Get combined embedding\n",
    "    combined_emb = get_combined_embedding(generated_image, clip_processor, clip_model, vae, vae_transform, device)\n",
    "    combined_emb_tensor = combined_emb.to(device)\n",
    "\n",
    "    # Predict artist labels\n",
    "    with torch.no_grad():\n",
    "        logits = model(combined_emb_tensor.unsqueeze(0))\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        top_probs, top_labels = torch.topk(probabilities, top_k)\n",
    "\n",
    "    # Decode labels\n",
    "    predicted_artists = label_encoder.inverse_transform(top_labels.cpu().numpy().flatten())\n",
    "\n",
    "    # Prepare attribution\n",
    "    attribution = []\n",
    "    for i in range(top_k):\n",
    "        attribution.append({\n",
    "            \"artist\": predicted_artists[i],\n",
    "            \"probability\": top_probs[0][i].item()\n",
    "        })\n",
    "\n",
    "    return generated_image, attribution\n",
    "\n",
    "def verify_external_image_enhanced(image_path, model, clip_processor, clip_model, vae, vae_transform, device, label_encoder, top_k=5):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Get combined embedding\n",
    "    combined_emb = get_combined_embedding(image, clip_processor, clip_model, vae, vae_transform, device)\n",
    "    combined_emb_tensor = combined_emb.to(device)\n",
    "\n",
    "    # Predict artist labels\n",
    "    with torch.no_grad():\n",
    "        logits = model(combined_emb_tensor.unsqueeze(0))\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        top_probs, top_labels = torch.topk(probabilities, top_k)\n",
    "\n",
    "    # Decode labels\n",
    "    predicted_artists = label_encoder.inverse_transform(top_labels.cpu().numpy().flatten())\n",
    "\n",
    "    # Prepare verification report\n",
    "    verification_report = []\n",
    "    for i in range(top_k):\n",
    "        verification_report.append({\n",
    "            \"artist\": predicted_artists[i],\n",
    "            \"probability\": top_probs[0][i].item()\n",
    "        })\n",
    "\n",
    "    return verification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the models (if needed)\n",
    "# Load classifier\n",
    "model = ArtistClassifier(input_dim=input_dim, num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load(\"artist_classifier.safetensors\"))\n",
    "model.eval()\n",
    "\n",
    "# Load fine-tuned Stable Diffusion\n",
    "sd_finetuned_pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "  sd_output_dir,\n",
    "  torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "sd_finetuned_pipeline.to(device)\n",
    "\n",
    "# Load label encoder\n",
    "with open(\"label_encoder.pkl\", \"rb\") as f:\n",
    "  label_encoder = pickle.load(f)\n",
    "\n",
    "# Example Prompt\n",
    "prompt = \"A futuristic cityscape at sunset\"\n",
    "generated_image, attribution = generate_image_with_artist_reference(\n",
    "  prompt=prompt,\n",
    "  sd_pipeline=sd_finetuned_pipeline,\n",
    "  model=model,\n",
    "  clip_processor=clip_processor,\n",
    "  clip_model=clip_model,\n",
    "  vae=vae,\n",
    "  vae_transform=vae_transform,\n",
    "  label_encoder=label_encoder,\n",
    "  device=device,\n",
    "  tokenizer=tokenizer,\n",
    "  top_k=3\n",
    ")\n",
    "\n",
    "# Display the generated image\n",
    "generated_image.show()\n",
    "\n",
    "# Print attribution\n",
    "print(\"\\nAttribution:\")\n",
    "for idx, attrib in enumerate(attribution, 1):\n",
    "  print(f\"{idx}. Artist: {attrib['artist']}, Probability: {attrib['probability']:.2f}\")\n",
    "\n",
    "# Example External Image Verification\n",
    "external_image_path = \"path_to_external_image.jpg\"  # Replace with your image path\n",
    "verification_report = verify_external_image_enhanced(\n",
    "  image_path=external_image_path,\n",
    "  model=model,\n",
    "  clip_processor=clip_processor,\n",
    "  clip_model=clip_model,\n",
    "  vae=vae,\n",
    "  vae_transform=vae_transform,\n",
    "  device=device,\n",
    "  label_encoder=label_encoder,\n",
    "  top_k=5\n",
    ")\n",
    "\n",
    "if verification_report:\n",
    "  print(\"\\nVerification Report:\")\n",
    "  for idx, report in enumerate(verification_report, 1):\n",
    "    print(f\"{idx}. Artist: {report['artist']}, Probability: {report['probability']:.2f}\")\n",
    "else:\n",
    "  print(\"\\nNo similar images found or error in processing.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
