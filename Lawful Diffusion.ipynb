{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from safetensors.torch import save_file\n",
    "from huggingface_hub import HfApi, upload_folder\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "sd_pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    sd_model_id,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "sd_pipeline = sd_pipeline.to(device)\n",
    "vae = sd_pipeline.vae\n",
    "vae.to(device)\n",
    "vae.eval()\n",
    "\n",
    "clip_model_id = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(clip_model_id)\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_model_id)\n",
    "clip_model = clip_model.to(device)\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define Transformation for VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_transform = Compose([\n",
    "    Resize(512, interpolation=Image.BICUBIC),\n",
    "    CenterCrop(512),\n",
    "    ToTensor(),\n",
    "    Normalize([0.5], [0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define Embedding Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_embedding(image, clip_processor, clip_model, device):\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        clip_features = clip_model.get_image_features(**inputs)\n",
    "    clip_features = clip_features / clip_features.norm(p=2, dim=-1, keepdim=True)\n",
    "    return clip_features.cpu().numpy()\n",
    "\n",
    "def get_vae_embedding(image, vae, vae_transform, device):\n",
    "    image = vae_transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        latent_dist = vae.encode(image).latent_dist\n",
    "        vae_features = latent_dist.mean\n",
    "    return vae_features.cpu().numpy()\n",
    "\n",
    "def get_combined_embedding(image, clip_processor, clip_model, vae, vae_transform, device):\n",
    "    clip_emb = get_clip_embedding(image, clip_processor, clip_model, device)\n",
    "    vae_emb = get_vae_embedding(image, vae, vae_transform, device)\n",
    "    combined_emb = np.concatenate([clip_emb, vae_emb], axis=1)\n",
    "    return combined_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define Custom Dataset using Hugging Face datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtistDataset(Dataset):\n",
    "    def __init__(self, dataset_name, split, clip_processor, clip_model, vae, vae_transform, label_encoder):\n",
    "        self.dataset = load_dataset(dataset_name, split=split)\n",
    "        self.clip_processor = clip_processor\n",
    "        self.clip_model = clip_model\n",
    "        self.vae = vae\n",
    "        self.vae_transform = vae_transform\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['image']\n",
    "        artist = item['artist']\n",
    "\n",
    "        # Get combined embedding\n",
    "        combined_emb = get_combined_embedding(image, self.clip_processor, self.clip_model, self.vae, self.vae_transform, device)\n",
    "\n",
    "        # Get label\n",
    "        label = self.label_encoder.transform([artist])[0]\n",
    "\n",
    "        return torch.tensor(combined_emb, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Initialize Label Encoder and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"your_dataset_name\"  # Replace with your Hugging Face dataset name\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Gather artist names\n",
    "artist_names = dataset['train'].unique('artist')\n",
    "\n",
    "# Initialize and fit LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(artist_names)\n",
    "\n",
    "# Save LabelEncoder\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_workers = 4\n",
    "\n",
    "train_dataset = ArtistDataset(\n",
    "    dataset_name=dataset_name,\n",
    "    split='train',\n",
    "    clip_processor=clip_processor,\n",
    "    clip_model=clip_model,\n",
    "    vae=vae,\n",
    "    vae_transform=vae_transform,\n",
    "    label_encoder=label_encoder\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Define the Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class ArtistClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ArtistClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc3(x)\n",
    "        return logits\n",
    "\n",
    "# Determine input dimensions\n",
    "clip_dim = 512\n",
    "vae_dim = 512\n",
    "input_dim = clip_dim + vae_dim\n",
    "\n",
    "num_classes = len(artist_names)\n",
    "\n",
    "model = ArtistClassifier(input_dim=input_dim, num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Define Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Save the Trained Model using safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(model.state_dict(), \"artist_classifier.safetensors\")\n",
    "print(\"Model saved to artist_classifier.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Upload the model to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "repo_id = \"your-username/your-model-name\"  # Replace with your desired repository name\n",
    "\n",
    "# Create the repository if it doesn't exist\n",
    "api.create_repo(repo_id, exist_ok=True)\n",
    "\n",
    "# Upload the model file\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"artist_classifier.safetensors\",\n",
    "    path_in_repo=\"artist_classifier.safetensors\",\n",
    "    repo_id=repo_id,\n",
    ")\n",
    "\n",
    "# Upload the label encoder\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"label_encoder.pkl\",\n",
    "    path_in_repo=\"label_encoder.pkl\",\n",
    "    repo_id=repo_id,\n",
    ")\n",
    "\n",
    "print(f\"Model and label encoder uploaded to {repo_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Load the Trained Model (for inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ArtistClassifier(input_dim=input_dim, num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load(\"artist_classifier.safetensors\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Define Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_with_artist_reference(prompt, sd_pipeline, model, clip_processor, clip_model, vae, vae_transform, label_encoder, device, top_k=3):\n",
    "    # Generate Image\n",
    "    with torch.autocast(device.type):\n",
    "        generated_image = sd_pipeline(prompt).images[0]\n",
    "\n",
    "    # Get combined embedding\n",
    "    combined_emb = get_combined_embedding(generated_image, clip_processor, clip_model, vae, vae_transform, device)\n",
    "    combined_emb_tensor = torch.tensor(combined_emb, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Predict artist labels\n",
    "    with torch.no_grad():\n",
    "        logits = model(combined_emb_tensor)\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        top_probs, top_labels = torch.topk(probabilities, top_k)\n",
    "\n",
    "    # Decode labels\n",
    "    predicted_artists = label_encoder.inverse_transform(top_labels.cpu().numpy().flatten())\n",
    "\n",
    "    # Prepare attribution\n",
    "    attribution = []\n",
    "    for i in range(top_k):\n",
    "        attribution.append({\n",
    "            \"artist\": predicted_artists[i],\n",
    "            \"probability\": top_probs[0][i].item()\n",
    "        })\n",
    "\n",
    "    return generated_image, attribution\n",
    "\n",
    "def verify_external_image_enhanced(image_path, model, clip_processor, clip_model, vae, vae_transform, device, label_encoder, top_k=5):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Get combined embedding\n",
    "    combined_emb = get_combined_embedding(image, clip_processor, clip_model, vae, vae_transform, device)\n",
    "    combined_emb_tensor = torch.tensor(combined_emb, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Predict artist labels\n",
    "    with torch.no_grad():\n",
    "        logits = model(combined_emb_tensor)\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        top_probs, top_labels = torch.topk(probabilities, top_k)\n",
    "\n",
    "    # Decode labels\n",
    "    predicted_artists = label_encoder.inverse_transform(top_labels.cpu().numpy().flatten())\n",
    "\n",
    "    # Prepare verification report\n",
    "    verification_report = []\n",
    "    for i in range(top_k):\n",
    "        verification_report.append({\n",
    "            \"artist\": predicted_artists[i],\n",
    "            \"probability\": top_probs[0][i].item()\n",
    "        })\n",
    "\n",
    "    return verification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Prompt\n",
    "prompt = \"A futuristic cityscape at sunset\"\n",
    "generated_image, attribution = generate_image_with_artist_reference(\n",
    "  prompt=prompt,\n",
    "  sd_pipeline=sd_pipeline,\n",
    "  model=model,\n",
    "  clip_processor=clip_processor,\n",
    "  clip_model=clip_model,\n",
    "  vae=vae,\n",
    "  vae_transform=vae_transform,\n",
    "  label_encoder=label_encoder,\n",
    "  device=device,\n",
    "  top_k=3\n",
    ")\n",
    "\n",
    "# Display the generated image\n",
    "generated_image.show()\n",
    "\n",
    "# Print attribution\n",
    "print(\"\\nAttribution:\")\n",
    "for idx, attrib in enumerate(attribution, 1):\n",
    "  print(f\"{idx}. Artist: {attrib['artist']}, Probability: {attrib['probability']:.2f}\")\n",
    "\n",
    "# Example External Image Verification\n",
    "external_image_path = \"path_to_external_image.jpg\"  # Replace with your image path\n",
    "verification_report = verify_external_image_enhanced(\n",
    "  image_path=external_image_path,\n",
    "  model=model,\n",
    "  clip_processor=clip_processor,\n",
    "  clip_model=clip_model,\n",
    "  vae=vae,\n",
    "  vae_transform=vae_transform,\n",
    "  device=device,\n",
    "  label_encoder=label_encoder,\n",
    "  top_k=5\n",
    ")\n",
    "\n",
    "if verification_report:\n",
    "  print(\"\\nVerification Report:\")\n",
    "  for idx, report in enumerate(verification_report, 1):\n",
    "    print(f\"{idx}. Artist: {report['artist']}, Probability: {report['probability']:.2f}\")\n",
    "else:\n",
    "  print(\"\\nNo similar images found or error in processing.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
